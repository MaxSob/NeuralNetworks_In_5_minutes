{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in 5 minutes\n",
    "In this notebook we will see how to implement a simple Neural Network from Bare Back code using numpy. The notation and backpropagation step is inspired in the amazing course of Andrew NG: [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning), in Coursera. It's a great material to start with deep Neural Networks.\n",
    "\n",
    "As an overview we will be reviewing the following topics:\n",
    "- Initalization\n",
    "- Forward Propagation\n",
    "- Error computing\n",
    "- Backward Propagation\n",
    "- Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Forward propagation\n",
    "The forward propagation step is done using the activations of the previous layer and multiplying them by the weights in the current layer, and then applying the activation function.\n",
    "We will use the sigmoid activation function as the main activation in all layers.\n",
    "We can write the computations as:\n",
    "$$\n",
    "Z^{(i)} = W_i A_{i-1} + b_i \\\\\n",
    "A^{(i)} = g(Z^{(i)})\n",
    "$$\n",
    "Where we have the following parameters: \n",
    "- $W_i$ weights of layer $i$\n",
    "- $A_{i-1}$ activations of the previous layer\n",
    "- $b_i$ bias vector of layer $i$\n",
    "- $g$ activation function of the layer $i$\n",
    "\n",
    "Let's define the sigmoid function and the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def forward_neuron(weights, previous_activations, bias, g):\n",
    "    Z = np.dot(weights, previous_activations) + bias\n",
    "    A = g(Z)\n",
    "    return Z, A\n",
    "\n",
    "def forward_propagation(X, theta, activation, cache={}):\n",
    "    previous_activation = X\n",
    "    cache[\"activation_layer_0\"] = X\n",
    "    layers = len(theta)\n",
    "    \n",
    "    for layer in range(len(theta)):\n",
    "        parameters = theta[layer]\n",
    "        bias = parameters[\"bias_layer_\" + str(layer + 1)]\n",
    "        weights = parameters[\"weights_layer_\" + str(layer + 1)]\n",
    "        Z, A = forward_neuron(weights, previous_activation, bias, activation)\n",
    "        cache[\"activation_layer_\" + str(layer + 1)] = A\n",
    "        cache[\"linear_layer_\" + str(layer + 1)] = Z\n",
    "        previous_activation = A\n",
    "        \n",
    "    return cache, previous_activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Error calculation\n",
    "The error calculation step is done computing the cost function which will be the average of the losses for our training examples.\n",
    "We will use the binary cross entropy loss function to compute the error of the function with respect of the true labels.\n",
    "The binary cross entropy loss is defined as:\n",
    "$$\n",
    "L(y, \\hat{y}) = y\\hspace{0.3em}log(\\hat{y}) + (1-y)\\hspace{0.3em}log(1 - \\hat{y})\n",
    "$$\n",
    "Where we have the parameters:\n",
    "- $y$ true label of our example\n",
    "- $\\hat{y}$ prediction of our neural network\n",
    "\n",
    "When we compute the loss for many examples we get the cost function of our paramters with respect of our dataset.\n",
    "Our cost function $J$ can be written as:\n",
    "$$\n",
    "J(X, Y; \\theta) = \\frac{1}{m} \\sum_{i=1}^m L(y_i, \\hat{y_i})\n",
    "$$\n",
    "Where we have the following elements:\n",
    "- $X$ the matrix of our input examples\n",
    "- $Y$ the matrix of our true labels predictions\n",
    "- $\\theta$ the learnable parameters of our network, i.e. the weights and biases of all the layers\n",
    "- $y_i$ ith label in our training set\n",
    "- $\\hat{y_i}$ ith prediction of our network\n",
    "\n",
    "As the formula can look scary it's a simple average over our examples.\n",
    "\n",
    "Let's define our loss and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def binary_cross_entropy(y, y_hat):\n",
    "    return y*np.log(y_hat) + (1 - y)*np.log(1 - y_hat)\n",
    "\n",
    "# Cost function given the predictions\n",
    "def cost_function_2(y_hat, y, loss):\n",
    "    return -1*np.mean(loss(y, y_hat))\n",
    "\n",
    "# Cost function given the inputs\n",
    "def cost_function(X, Y, theta, loss):\n",
    "    _, Y_hat = predict(X, theta, sigmoid)\n",
    "    return -1 *np.mean(loss(Y, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Back propagation\n",
    "The back propagation step, takes the derivative of the learnable parameters with respect of the cost function. This is called the gradient vector and points in the direction the cost function is growing most quickly. To compute this vector we need to use the chain rule of calculus:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i}\n",
    "$$\n",
    "The first step we need to perform to make the back propagation is to get the gradient of the cost with respect to the last activation. Since we are using a sigmoid funciton we have the following derivative:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+ e^{-x}}\\\\\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\\\\\n",
    "$$\n",
    "So the derivative in terms of the cost is:\n",
    "$$\n",
    "\\frac{\\partial J(y, \\hat{y}; \\theta)}{\\partial A} = \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}}\n",
    "$$\n",
    "To get the backward propagation of the output with respect to the paramter $w_i$ we need to compute:\n",
    "$$\n",
    "\\frac{\\partial J(y, \\hat{y}; \\theta)}{\\partial w_i} = \\Big( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\Big) \\sigma'(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def sigmoid_backward(activation_derivative, z):\n",
    "    return activation_derivative * sigmoid_derivative(z)\n",
    "\n",
    "def linear_gradient(linear_derivative, activation_parameters):\n",
    "    previous_activation, weights, bias = activation_parameters\n",
    "    m = previous_activation.shape[1]\n",
    "    weights_derivative = 1/m * np.dot(linear_derivative, previous_activation.T)\n",
    "    bias_derivative = 1/m * np.sum(linear_derivative, axis=1, keepdims=True)\n",
    "    previous_activation_derivative = np.dot(weights.T, linear_derivative)\n",
    "    return previous_activation_derivative, weights_derivative, bias_derivative\n",
    "\n",
    "def activation_gradient(activation_derivative, linear_activation, activation_parameters):\n",
    "    linear_derivative = sigmoid_backward(activation_derivative, linear_activation)\n",
    "    previous_activation_derivative, weights_derivative, bias_derivative = linear_gradient(linear_derivative, activation_parameters)\n",
    "    return previous_activation_derivative, weights_derivative, bias_derivative\n",
    "\n",
    "def gradient_layer(cache, layer):\n",
    "    l_name = str(layer)\n",
    "    activation_derivative = cache[\"activation_derivative_layer_\" + l_name]\n",
    "    weights = cache[\"weights_layer_\" + str(layer)]\n",
    "    bias = cache[\"bias_layer_\" + str(layer)]\n",
    "    prev_activation = cache[\"activation_layer_\" + str(layer)]\n",
    "    linear_activation = cache['linear_layer_' + str(layer)]\n",
    "    current_layer_parameters = (prev_activation, weights, bias)\n",
    "    previous_activation_derivative, weights_derivative, bias_derivative = activation_gradient(activation_derivative, linear_activation, current_layer_parameters)\n",
    "    cache[\"activation_derivative_layer_\" + str(layer - 1)] = previous_activation_derivative\n",
    "    cache[\"weights_derivative_layer_\" + l_name] = weights_derivative\n",
    "    cache[\"bias_derivative_layer_\" + l_name] = bias_derivative\n",
    "    return cache\n",
    "    \n",
    "def backward_propagation(y_hat, y, parameters, layers=2):\n",
    "    cache = {**parameters}\n",
    "    m = y_hat.shape[1]\n",
    "    y = y.reshape(y_hat.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    y_hat_derivative = - (np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat)) \n",
    "    cache[\"activation_derivative_layer_\" + str(layers)] = y_hat_derivative\n",
    "    \n",
    "    for layer in range(layers, 0, -1):\n",
    "        cache = gradient_layer(cache, layer)\n",
    "        \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update parameters\n",
    "\n",
    "To update the parameters at time $i$ we need to compute the gradient of the parameters and substract a portion of the gradient vector to the current weights:\n",
    "$$\n",
    "\\nabla_{\\theta}  = \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial J}{\\partial w_1} \\\\\n",
    "        \\frac{\\partial J}{\\partial w_2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial J}{\\partial w_n} \\\\\n",
    "    \\end{bmatrix}\\\\\n",
    "\\theta_i = \\theta_{i-1} - \\alpha \\nabla_{\\theta} J\n",
    "$$\n",
    "The portion of the gradient substracted to the learnable paramters is called learning rate and it is one of the hyper parameters of the algorithm denoted by $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(cache, alpha, layers=2):\n",
    "    for layer in range(layers):\n",
    "        layer_name = str(layer + 1)\n",
    "        cache[\"weights_layer_\" + layer_name] -= alpha * cache[\"weights_derivative_layer_\" + layer_name]\n",
    "        cache[\"bias_layer_\" + layer_name] -= alpha * cache[\"bias_derivative_layer_\" + layer_name]\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Putting all together\n",
    "Let's use our implemented methods to build our neural network\n",
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.seed(3)\n",
    "\n",
    "def initialize(layers_dimensions):\n",
    "    parameters = {}\n",
    "    layers = len(layers_dimensions)            # number of layers in the network\n",
    "    for layer in range(1, layers):\n",
    "        current_layer_dim = layers_dimensions[layer]\n",
    "        previous_layer_dim = layers_dimensions[layer - 1]\n",
    "        parameters['weights_layer_' + str(layer)] = np.random.randn(current_layer_dim, previous_layer_dim) * 0.01\n",
    "        parameters['bias_layer_' + str(layer)] = np.zeros((current_layer_dim, 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =====> 0.6938988007243275 ======== epoch 0\n",
      "cost =====> 0.6342570398826515 ======== epoch 500\n",
      "cost =====> 0.632044787537711 ======== epoch 1000\n",
      "cost =====> 0.6287053780357208 ======== epoch 1500\n",
      "cost =====> 0.6243443468541711 ======== epoch 2000\n",
      "cost =====> 0.618849243807791 ======== epoch 2500\n",
      "cost =====> 0.6112436756910246 ======== epoch 3000\n",
      "cost =====> 0.5991689915987959 ======== epoch 3500\n",
      "cost =====> 0.5772064548656747 ======== epoch 4000\n",
      "cost =====> 0.5311651263913292 ======== epoch 4500\n",
      "cost =====> 0.4249370261101307 ======== epoch 5000\n",
      "cost =====> 0.24841422785639106 ======== epoch 5500\n",
      "cost =====> 0.135103810173985 ======== epoch 6000\n",
      "cost =====> 0.09202899597047219 ======== epoch 6500\n",
      "cost =====> 0.07368617364434649 ======== epoch 7000\n",
      "cost =====> 0.06433509772364936 ======== epoch 7500\n",
      "cost =====> 0.05893573419458658 ======== epoch 8000\n",
      "cost =====> 0.05555952737763003 ======== epoch 8500\n",
      "cost =====> 0.05334422207594495 ======== epoch 9000\n",
      "cost =====> 0.051855505539745037 ======== epoch 9500\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "#Let's build a binary problem\n",
    "y = (iris.target > 0)*np.ones(iris.target.shape)\n",
    "cache = initialize([4,4,1])\n",
    "\n",
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    theta = []\n",
    "    #Define theta\n",
    "    theta.append({\"weights_layer_1\": cache['weights_layer_1'], \"bias_layer_1\": cache['bias_layer_1']})\n",
    "    theta.append({\"weights_layer_2\": cache['weights_layer_2'], \"bias_layer_2\": cache['bias_layer_2']})\n",
    "    #Forward propagation\n",
    "    cache, output = forward_propagation(X.T, theta, sigmoid, cache)\n",
    "    #Error calculation\n",
    "    cost = cost_function_2(output, y, binary_cross_entropy)\n",
    "    if i % 500 == 0:\n",
    "        print(\"cost =====> \" + str(cost) + \" ======== epoch \" + str(i))\n",
    "    #Gradient compute\n",
    "    cache = backward_propagation(output, y, cache)\n",
    "    #Weights update\n",
    "    cache = update_parameters(cache, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02273728 0.00598972 0.00453463 0.00489739 0.02189242 0.27073357\n",
      "  0.00678968 0.0192699  0.00211211 0.00840798 0.09345771 0.01578738\n",
      "  0.00467889 0.00119507 0.20048741 0.62781245 0.09328273 0.02428298\n",
      "  0.39531849 0.05854096 0.08744083 0.05102079 0.00375849 0.04168269\n",
      "  0.03464637 0.01172003 0.0285716  0.03769389 0.02361791 0.00883992\n",
      "  0.00914595 0.05755895 0.12173509 0.24709919 0.00891475 0.00652266\n",
      "  0.04532442 0.01632597 0.00201811 0.02430433 0.01493353 0.00103579\n",
      "  0.00265578 0.04002564 0.18879501 0.00520806 0.07214191 0.00464014\n",
      "  0.07287115 0.01242257 0.99998805 0.99995431 0.99998916 0.98261906\n",
      "  0.99994177 0.99944395 0.99997053 0.40994109 0.99995475 0.97975395\n",
      "  0.4022209  0.99955687 0.99374142 0.99990538 0.98919194 0.99996049\n",
      "  0.99959661 0.99766351 0.99951683 0.98721365 0.99994658 0.9991438\n",
      "  0.99992631 0.99987109 0.9998598  0.99994352 0.99997681 0.9999872\n",
      "  0.99981487 0.96828797 0.96801312 0.95077124 0.99616349 0.99994235\n",
      "  0.99931853 0.99993118 0.99997927 0.99953596 0.99841723 0.98964311\n",
      "  0.99772317 0.99989934 0.99647494 0.42200896 0.99763816 0.99905456\n",
      "  0.99889716 0.99977572 0.39178162 0.99808008 0.9999968  0.99992686\n",
      "  0.99999755 0.99999057 0.99999549 0.99999882 0.99172706 0.99999824\n",
      "  0.99999348 0.99999862 0.99998986 0.99998371 0.99999517 0.99984129\n",
      "  0.99995337 0.9999923  0.99999246 0.99999913 0.99999889 0.99980343\n",
      "  0.99999713 0.99985495 0.99999883 0.99995551 0.99999666 0.99999802\n",
      "  0.99994272 0.99995965 0.99999174 0.99999724 0.99999802 0.99999908\n",
      "  0.99999203 0.99997064 0.99997794 0.99999858 0.99999533 0.99999231\n",
      "  0.99993883 0.99999552 0.99999598 0.9999939  0.99992686 0.9999974\n",
      "  0.99999697 0.99999249 0.99995309 0.99998904 0.99999302 0.99996146]]\n"
     ]
    }
   ],
   "source": [
    "theta = []\n",
    "theta.append({\"weights_layer_1\": cache['weights_layer_1'], \"bias_layer_1\": cache['bias_layer_1']})\n",
    "theta.append({\"weights_layer_2\": cache['weights_layer_2'], \"bias_layer_2\": cache['bias_layer_2']})\n",
    "_, output = forward_propagation(X.T, theta, sigmoid, cache)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (output > 0.5) * np.ones(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      "  1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "correct = (predictions == y) * np.ones(y.shape)\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(correct)/len(correct[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
